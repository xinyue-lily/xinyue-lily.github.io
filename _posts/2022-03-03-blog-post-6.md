---
layout: post
title: Blog Post 6
---

2022-03-03

# Fake News Classification

## The Task
We will develop and assess a fake news classifier using Tensorflow.

This is the link to my github respository. [https://github.com/xinyudong1129/Blog-Post/tree/main/blogpost6](https://github.com/xinyudong1129/Blog-Post/tree/main/blogpost6)

## Reference website
[Text classification](https://www.tensorflow.org/tutorials/keras/text_calssification)
  
[Functional API](https://www.tensorflow.org/guide/keras/functional)

[stop words](https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe)

[Text Vectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization)

## 1. Obtain Data

### (1) DataSource

Our data comes from the article
- Ahmed H, Traore I, Saad S.(2017)"Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A.(eds) *Intelligent, Secure,and Dependable Systems in Distributed and Cloud Environments.* ISDDC 2017. *Lecture Notes in Computer Science*, vol 10618. Springer, Cham (pp.127-138)

- The professor have done a small amount of data cleaning and performed a train-test split.

### (2) Make a Dataset
We write a function called make_dataset . This function do two things:

- Remove stopwords from the article text and title . A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may 􀃘nd this StackOverFlow thread to be helpful.

- Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (title, text) , and the output should consist only of the fake column. 

Because we have multiple inputs, we are going to construct our Dataset from a tuple of dictionaries. The first dictionary is going to specify the different components in the predictor data, while the second dictionary is going to specify the different components of the target data.

```python
from nltk.corpus import stopwords
stop = stopwords.words('english')
def make_dataset(df):
    df["title"] = df["title"].apply(lambda x: ' '.join([word for word in 
                                    x.split() if word not in (stop)]))
    df["text"] = df["text"].apply(lambda x: ' '.join([word for word in 
                                  x.split() if word not in (stop)]))
    data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]],
            "text" : df[["text"]]
        }, 
        {
            "fake" : df[["fake"]]
        }
       )
    )
    return data
```

### (3) Perform train/test/vaidation split
Call the function make_dataset on my training dataframe to produce a train_dataset, then split 20% of it to use for validation_dataset. 

Call the funciton make_dataset on my testing dataframe to produce a test_dataset.

For each of the three Datasets, we batch them into small chunks of data. This can sometimes reduce accuracy, but can also greatly increase the speed of training. I found batches of 100 rows to work well.

```python
df1 = pd.read_csv("fake_news_train.csv")
data = make_dataset(df1)
data = data.shuffle(buffer_size = len(data))
train_size = int(0.8*len(data))
train_dataset = data.take(train_size).batch(100)
val_dataset = data.skip(train_size).batch(100)
df2 = pd.read_csv("fake_news_test.csv")
test_dataset = make_dataset(df2).batch(100)

len(train_dataset),len(val_dataset),len(test_dataset)
```
```python
(180, 45, 225)
```

## 2. Create Models

In this part, we create three Tensorflow models to compare their performance.

- In the first model, I use only the article title as an input.
- In the second model, I use only the article text as an input.
- In the third model, I use both the article title and the article text as input.

### (1) Standardization and Vectorization
*Standardization* refers to the act of taking a some text that's "messy" in some way and making it lesss messy. Common standardizations includes:
- Removing capitals.
- Removing punctuation.
- Removing HTML elements or other non-semantic content.
In this standardization, we convert all text to lowercase and remove punctuation.

```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                        '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 
```
*Vectorization* refers to the process of representing text as a vector (array, tensor). There are multiple ways to carry out vectorization. We replace each word by its *frequency rank* in the data. 

```python
vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=25) 
vectorize_layer.adapt(train_dataset.map(lambda x, y: x["title"]))
```

We adapt the vectorizaiton layer to the title. In the adaption process, the vectorization layer learns what words are common in title.

```python
vectorize_layer1 = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500)
vectorize_layer1.adapt(train_dataset.map(lambda x, y: x["text"]))
```
We adapt the vectorizaiton layer to the text. In the adaption process, the vectorization layer learns what words are common in text.

### (2) Inputs
```python
# inputs
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

### (3) Model1: only the article title as an input

In the first model, We write a pipeline for the title. This pipeline include one vectorize layer, one Embedding layer, one GlobalAveragePooling1D layer, one dense layer, two Dropout layer.

```python
# layers for processing the title
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 2, 
                                   name = "embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='sigmoid')(title_features)
title_output = layers.Dense(2,name="fake")(title_features)
model1 = keras.Model(
    inputs = [title_input], 
    outputs = title_output
)
```
Train model1 and plot the history of the accuracy on both the training and vailidation sets.

```python
model1.compile(optimizer = "adam",
            loss = losses.SparseCategoricalCrossentropy(from_logits=True),
            metrics=['accuracy']
)
history = model1.fit(train_dataset, 
                validation_data=val_dataset,
                epochs = 10) 
```
Here's the history of the accuracy on both the training and vailidation sets.

```python
Epoch 1/10
180/180 [==============================] - 1s 8ms/step - loss: 0.6852 
         - accuracy: 0.5496 - val_loss: 0.6596 - val_accuracy: 0.5256
Epoch 2/10
180/180 [==============================] - 1s 7ms/step - loss: 0.6051 
         - accuracy: 0.8028 - val_loss: 0.5268 - val_accuracy: 0.9356
Epoch 3/10
180/180 [==============================] - 1s 7ms/step - loss: 0.4523 
         - accuracy: 0.8695 - val_loss: 0.3466 - val_accuracy: 0.9339
Epoch 4/10
180/180 [==============================] - 1s 7ms/step - loss: 0.3275 
         - accuracy: 0.9003 - val_loss: 0.2369 - val_accuracy: 0.9501
Epoch 5/10
180/180 [==============================] - 1s 7ms/step - loss: 0.2512 
         - accuracy: 0.9184 - val_loss: 0.1840 - val_accuracy: 0.9488
Epoch 6/10
180/180 [==============================] - 1s 7ms/step - loss: 0.2098 
         - accuracy: 0.9281 - val_loss: 0.1395 - val_accuracy: 0.9670
Epoch 7/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1761 
         - accuracy: 0.9366 - val_loss: 0.1255 - val_accuracy: 0.9641
Epoch 8/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1581 
         - accuracy: 0.9435 - val_loss: 0.1062 - val_accuracy: 0.9686
Epoch 9/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1433 
         - accuracy: 0.9472 - val_loss: 0.0832 - val_accuracy: 0.9780
Epoch 10/10
180/180 [==============================] - 1s 7ms/step - loss: 0.1261 
         - accuracy: 0.9531 - val_loss: 0.0759 - val_accuracy: 0.9780
```

Plot the history of model1, including the training and validation performance.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![PIC2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/pic2_model1_history.png)

#### My observations
(1) The validation accuracy of model1 stabilized between **52% and 97.8%** during training.

(2) The highest validation accuracy is **97.8%**. 

(3) From the plot, I observed overfitting in model1. For example, in epoch 7/20, we got the highest validation accuracy 63.74% , but then the validation accuracy dropped a little.


### (4) Model2: Only the article text as an input

We write a pipeline for the text. This pipeline include one vectorize layer, one Embedding layer, one GlobalAveragePooling1D layer, one dense layer, two Dropout layer.

```python
text_features = vectorize_layer1(text_input)
text_features = layers.Embedding(size_vocabulary, 2, 
                                 name = "embedding1")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='sigmoid')(text_features)
text_output = layers.Dense(2,name="fake")(text_features)
model2 = keras.Model(
    inputs = [text_input], 
    outputs = text_output
)
```
Train model2 and plot the history of the accuracy on both the training and vailidation sets.

```python
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
history = model2.fit(train_dataset, 
                    validation_data=val_dataset,
                    epochs = 10) 
```
Here's the history of the accuracy on both the training and vailidation sets.

```python
Epoch 1/10
180/180 [==============================] - 8s 44ms/step - loss: 0.7272 
          - accuracy: 0.5121 - val_loss: 0.6877 - val_accuracy: 0.5383
Epoch 2/10
180/180 [==============================] - 8s 42ms/step - loss: 0.6785 
          - accuracy: 0.5767 - val_loss: 0.6619 - val_accuracy: 0.6637
Epoch 3/10
180/180 [==============================] - 8s 42ms/step - loss: 0.6379 
          - accuracy: 0.7740 - val_loss: 0.6106 - val_accuracy: 0.6782
Epoch 4/10
180/180 [==============================] - 8s 43ms/step - loss: 0.5636 
          - accuracy: 0.8670 - val_loss: 0.5117 - val_accuracy: 0.9254
Epoch 5/10
180/180 [==============================] - 8s 42ms/step - loss: 0.4697 
          - accuracy: 0.8997 - val_loss: 0.4099 - val_accuracy: 0.9356
Epoch 6/10
180/180 [==============================] - 8s 42ms/step - loss: 0.3843 
          - accuracy: 0.9055 - val_loss: 0.3294 - val_accuracy: 0.9434
Epoch 7/10
180/180 [==============================] - 8s 44ms/step - loss: 0.3212 
          - accuracy: 0.9151 - val_loss: 0.2735 - val_accuracy: 0.9314
Epoch 8/10
180/180 [==============================] - 7s 41ms/step - loss: 0.2796 
          - accuracy: 0.9231 - val_loss: 0.2437 - val_accuracy: 0.9421
Epoch 9/10
180/180 [==============================] - 8s 43ms/step - loss: 0.2526 
          - accuracy: 0.9285 - val_loss: 0.2129 - val_accuracy: 0.9555
Epoch 10/10
180/180 [==============================] - 8s 44ms/step - loss: 0.2314 
          - accuracy: 0.9339 - val_loss: 0.1862 - val_accuracy: 0.9552
```

Plot the history of model2, including the training and validation performance.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![PIC2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/pic2_model1_history.png)

#### My observations
(1) The validation accuracy of model1 stabilized between **56% and 64%** during training.

(2) The highest validation accuracy is **63.74%**. Compare that to the baseline, I improved 11% of accuracy.

(3) From the plot, I observed overfitting in model1. For example, in epoch 7/20, we got the highest validation accuracy 63.74% , but then the validation accuracy dropped a little.


### (5) Model3: both the article title and the article text as input

```python
main = layers.concatenate([title_features,text_features],axis=1)
output = layers.Dense(2,name="fake")(main)
model3 = keras.Model(
     inputs = [title_input,text_input],
     outputs = output
)
```

```python
keras.utils.plot_model(model3)
```


```python
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
history = model3.fit(train_dataset, 
                    validation_data=val_dataset,
                    epochs = 10) 
```
Here's the history of the accuracy on both the training and vailidation sets.

```python
Epoch 1/10
180/180 [==============================] - 9s 47ms/step - loss: 0.6823 
          - accuracy: 0.5917 - val_loss: 0.6542 - val_accuracy: 0.6523
Epoch 2/10
180/180 [==============================] - 8s 47ms/step - loss: 0.5899 
          - accuracy: 0.8386 - val_loss: 0.4951 - val_accuracy: 0.9501
Epoch 3/10
180/180 [==============================] - 8s 44ms/step - loss: 0.4185 
          - accuracy: 0.8852 - val_loss: 0.3066 - val_accuracy: 0.9588
Epoch 4/10
180/180 [==============================] - 8s 45ms/step - loss: 0.2909 
          - accuracy: 0.9139 - val_loss: 0.2036 - val_accuracy: 0.9619
Epoch 5/10
180/180 [==============================] - 8s 43ms/step - loss: 0.2100 
          - accuracy: 0.9370 - val_loss: 0.1534 - val_accuracy: 0.9604
Epoch 6/10
180/180 [==============================] - 8s 43ms/step - loss: 0.1607 
          - accuracy: 0.9528 - val_loss: 0.1035 - val_accuracy: 0.9766
Epoch 7/10
180/180 [==============================] - 8s 43ms/step - loss: 0.1298 
          - accuracy: 0.9627 - val_loss: 0.0857 - val_accuracy: 0.9784
Epoch 8/10
180/180 [==============================] - 8s 43ms/step - loss: 0.1083 
          - accuracy: 0.9714 - val_loss: 0.0707 - val_accuracy: 0.9840
Epoch 9/10
180/180 [==============================] - 8s 43ms/step - loss: 0.0920 
          - accuracy: 0.9746 - val_loss: 0.0620 - val_accuracy: 0.9831
Epoch 10/10
180/180 [==============================] - 8s 44ms/step - loss: 0.0770 
          - accuracy: 0.9801 - val_loss: 0.0475 - val_accuracy: 0.9878
```

Plot the history of model1, including the training and validation performance.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![PIC2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/pic2_model1_history.png)

#### My observations
(1) The validation accuracy of model1 stabilized between **56% and 64%** during training.

(2) The highest validation accuracy is **63.74%**. Compare that to the baseline, I improved 11% of accuracy.

(3) From the plot, I observed overfitting in model1. For example, in epoch 7/20, we got the highest validation accuracy 63.74% , but then the validation accuracy dropped a little.


## 3.Model Evaluation
we’ll test the model performance on unseen test data. 
```python
model1.evaluate(test_dataset)
```
```python
225/225 [==============================] - 1s 4ms/step - loss: 0.0883 
                                                   - accuracy: 0.9726
```
```python
[0.0883011594414711, 0.972649097442627]
```
```python
model2.evaluate(test_dataset)
```
```python
225/225 [==============================] - 6s 29ms/step - loss: 0.2057 
                                                    - accuracy: 0.9488
```
```python
[0.20567873120307922, 0.9488173127174377]
```
```python
model3.evaluate(test_dataset)
```
```python
225/225 [==============================] - 7s 29ms/step - loss: 0.0583 
                                                    - accuracy: 0.9837
```
```python
[0.05833631381392479, 0.9836518168449402]
```
### My observations

## 4.Embedding Visualizaiton

Visualize and comment on the embedding that our model learned. We are able to find some interesting patterns or associations in the words that the model found useful when distinguishing real news from fake news.

The collection of weights is 3-dimensional. For plotting in 2 dimensions, we have several choices for how to reduce the data to a 2d representation. A very simple and standard approach is our friend, principal component analysis (PCA).

```python
weights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```
Ready to plot! Note that the embedding appear to be “stretched out” in two directions, with one direction corresponding to fake news or not.

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
#### My observations:


Thank you!!!