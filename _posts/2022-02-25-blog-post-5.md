---
layout: post
title: Blog Post 5
---


2022-02-25

# Image Classification

## Tensorflow

The Blog Post 5 is working in Google Colab. When training my model, enabling GPU lead to significant speed benefits.

## The Task
Classify cats and dogs

This is the link to my github respository. [https://github.com/xinyudong1129/Blog-Post/tree/main/blogpost5](https://github.com/xinyudong1129/Blog-Post/tree/main/blogpost5)

## Reference website
[Transfer leaning tutorials](https://www.tensorflow.org/tutorials/images/transfer_learning)

[Transfer leaning guide](https://tensorflow.google.cn/guide/keras/transfer_learning)

[Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/)

[Data augmentation](https://tensorflow.google.cn/tutorials/images/data_augmentation)

[Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

## Load Packages and Obtain Data

Load Packages

```python

import os
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras import datasets, layers, models,utils
from tensorflow.keras.preprocessing import image_dataset_from_directory

```

We'll use the dataset provided by the Tensorflow team that contains labeled images of cats and dogs.

```python
# location of data, code provided by professsor.
_URL = 'https://storage.googleapis.com/mledu-datasets/
        cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                           shuffle=True,
                           batch_size=BATCH_SIZE,
                           image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                           shuffle=True,
                           batch_size=BATCH_SIZE,
                           image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)

```
We constructed three datasets: training_dataset, validation_dataset and test_dataset using image_dataset_from_directory.

```python
Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
68608000/68606236 [==============================] - 1s 0us/step
68616192/68606236 [==============================] - 1s 0us/step
Found 2000 files belonging to 2 classes.
Found 1000 files belonging to 2 classes.
```

The following code is the technical code related to rapidly reading data provided by the professor.

```python

AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

```
## 1.Visualiazation of images
We can get a piece of a dataset using the *take^ method; will retrieve one batch(32 images with labels) from the training dataset.

We try to create a two-row visualizaiton. In the first row, show three random pictures of cats. In the second row, show three random pictures of dogs. Here's my code.

```python

cat = 0
dog = 0
plt.figure(figsize=(10, 6))
for images, labels in train_dataset.take(1):
  for i in range(32):
    if labels[i].numpy().astype("uint8")==0 and cat < 3:
      cat = cat + 1
      ax = plt.subplot(2, 3, cat)
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title("cat")
     
    elif labels[i].numpy().astype("uint8")==1 and dog < 3:
      dog = dog + 1
      ax = plt.subplot(2, 3, dog+3)
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title("dog")
    plt.axis("off")
    if cat >= 3 and dog >= 3:
      break

```


That's not right!

We will try to implement *Spectral clustering* to correctly cluster the two crescents. 

This is the link to my github respository. [https://github.com/xinyue-lily/Blog-Post/tree/main/blogpost4](https://github.com/xinyue-lily/Blog-Post/tree/main/blogpost4)

## Part A

> Construct the similarity matrix 𝐀 use a parameter epsilon. Entry A[i,j] should be equal to 1 if X[i] is within distance epsilon of X[j], and 0 otherwise. The diagonal entries A[i,i] should all be equal to zero. 


```python

def get_similarity_matrix(X,epsilon):
    """
    X      : input data
    epsilon: distance threshold
    return : similarity matrix 𝐀 
    """
    dists = pairwise_distances(X, X)
    dists = np.where(dists > epsilon, 0, 1)
    np.fill_diagonal(dists,0)
    return dists

```

## Part B

> Construct the cut and volume terms. 𝐜𝐮𝐭 (C0, C1) is the cut of the clusters C0 and C1. 𝐯𝐨𝐥 (C) is the volume of cluster C which is a measure of the size of the cluster. then construct normcut(A,y) which uses cut(A,y) and vols(A,y) to compute the binary normalized cut objective of a matrix A with clustering vector y.

```python
def get_D(A):
    """
    A: similarity matrix 𝐀 
    return: the degree Matrix of A
    """
    D = np.diag(sum(A))
    return D

def cut(A,y):
    """
    A: similarity matrix 𝐀 
    y: clustering vector y
    return: the cut objective 𝐜𝐮𝐭(𝐶0,𝐶1) for y
    """
    B = A[y==0,:]
    B = B[:,y==1]
    return B.sum()

def vols(A,y):
    """
    A: similarity matrix 𝐀 
    y: clustering vector y
    return: computes the volumes of 𝐶0 and 𝐶1,returning them as a tuple.
    """
    d = np.sum(A,axis=0)
    v0 = d[y==0].sum()
    v1 = d[y==1].sum()
    return v0,v1

def normcut(A,y):
    """
    A: similarity matrix 𝐀 
    y: clustering vector y
    return: the binary normalized cut objective of a matrix A with 
            clustering vector y.
    """
    v0,v1 = vols(A,y)
    Na = cut(A,y)*(1/v0+1/v1)
    return Na

```

We generate a random vector of random labels of length n, with each label equal to either 0 or 1. Then we compare the cut objective and the normcut objective using both the true labels y and the random labels yr.
```python
n = 1000
#yr are the random labels of the points
yr = np.random.randint(0, 2, size = (n))

cut1 = cut(A,y)
cut2 = cut(A,yr)
print(cut1,cut2)

v0,v1= vols(A,y)
print(v0,v1)

na1 = normcut(A,y)
na2 = normcut(A,yr)
print(na1,na2)

```

```python
55 28106
30981 81733
0.0024482044338575517 0.9995105094347133 
```

From the results, the cut objective for the true labels is much smaller than the cut objective for the random labels, and the normcut objective for the true labels is much smaller than the normcut objective using the random labels.

## Part C
> Write a function called transform(A,y) to compute the appropriate 𝐳 vector.

![C1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/C1.png)

![C2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/C2.png)

```python
def transform(A,y):
    """
    A: similarity matrix 𝐀 
    y: clustering vector y
    return: the appropriate 𝐳 vector
    """
    v0,v1 = vols(A,y)
    z = np.where(y == 0, 1/v0, -1/v1)
    return z
    
"""
check the equation above that relates the matrix product to 
the normcut objective, by computing each side separately and 
checking that they are equal.
"""
L = D - A
na2 = (z.T @ L @ z)/(z.T @ D @ z)
na4 = (z.T @ L @ z)/(z @ D @ z)
#na2 and na4 is close
#normcut(A,y) and (z.T @ L @ z)/(z.T @ D @ z) is close
print(np.isclose(na2,na4),np.isclose(na2,na))
#check the identity 𝐳𝑇𝐃𝟙=0
na5 = z.T @ D @ np.ones(n)
print(np.isclose(na5,0))
```

```python
True  True   
True
```

## Part D

> The problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function.![D2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/D2.png)
subject to the condition 𝐳𝑇𝐃𝟙=0, 
We use the minimize function from scipy.optimize to minimize the function orth_obj with respect to  𝐳 . 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)

#Use the minimize function from scipy.optimize to minimize the function 
#orth_obj with respect to 𝐳,the initial value is random labels yr
#return the minimizing vector z_min
#the optimization is pretty slow,almost ten minutes
z_min = scipy.optimize.minimize(orth_obj,z)

```

![D1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/D1.png)

## Part E
> The plot shows the attempted data clustering from the optimization

```python
label = np.where(z_min.x>0,1,0)
plt.scatter(X[:,0],X[:,1],c=label)
```
![E1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/E1.png)

Just few mistakes.

## Part F
> The Laplacian matrix and its eigenvector corresponding to the second-smallest eigenvalue are both computed. The plot shows a more correct clustering of the data, with only a small number of points mis-clustered

```python
L = np.linalg.inv(D) @ (D-A)
eigval, eigvec = np.linalg.eig(L)
z_eig = eigval[1].real
print(z_eig)  
z_vec = np.real(eigvec[:,1])
plt.scatter(X[:,0], X[:,1], c = z_vec < 0)
```
```python
z_eig = 0.002204547657770541 z_min = 0.002211107428317444  
z_eig is close to z_min
```

![F3.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/F3.png)

## Part G
> We write a function called spectral_clustering(X, epsilon) which takes in the input data X (in the same format as Part A) and the distance threshold epsilon and performs spectral clustering, returning an array of binary labels indicating whether data point i is in group 0 or group 1. 

```python
def spectral_clustering(X, epsilon):
    """
    X      :input data
    epsilon:threshold param
    return :performs spectral clustering, returning an array of binary labels 
            indicating whether data point i is in group 0 or group 1. 
    """
    #Construct the similarity matrix.
    A = get_similarity_matrix(X,epsilon)
    #construct the degree matrix
    D = get_D(A)
    #Construct the Laplacian matrix
    L = np.linalg.inv(D) @ (D-A)
    #Compute the eigenvector with second-smallest eigenvalue of the 
    #Laplacian matrix.
    eigval, eigvec = np.linalg.eig(L)
    eigvec = np.real(eigvec)
    c = eigvec[:,1]
    #return labels based on this eigenvector.
    label = np.where(c>0,1,0)
    return  label

def plot(X,label):
    """
    plot the result
    X: input data
    label: the result of spectral clustering
    """
    plt.scatter(X[:,0],X[:,1],c=label)
    correct = np.isclose(y,label)
    plt.title('The correct rate:{:.0%}'.format(correct.sum()/1000) 
    
def correct_rate(X,y,label):
    """
    compute the correct rate
    X: input data
    y: true value
    label: the result of spectral clustering
    """
    correct = np.isclose(y,label)
    return 'the correct rate:{:.0%}'.format(correct.sum()/1000)

```
## Part H

> We generated different data sets using make_moons. When the noise increased, spectral clustering still find the two half-moon clusters.

```python 
n = 1000
plt.figure(figsize=(12,8))
X1,y1 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.subplot(221)
plt.title("noise=0.05")
plt.scatter(X1[:,0], X1[:,1])
X2,y2 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.08, random_state=None)
plt.subplot(222)
plt.title("noise=0.08")
plt.scatter(X2[:,0], X2[:,1])
X3,y3 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.subplot(223)
plt.title("noise=0.10")
plt.scatter(X3[:,0], X3[:,1])
X4,y4 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.12, random_state=None)
plt.subplot(224)
plt.title("noise=0.12")
plt.scatter(X4[:,0], X4[:,1]) 
```

![H1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/H1.png)


```python 
plt.figure(figsize=(12,8))

label1 = spectral_clustering(X1,0.4)
plt.subplot(221)
correct1 = correct_rate(X1,y1,label1)
plt.title("noise=0.05,"+correct1)
plt.scatter(X1[:,0],X1[:,1],c=label1)

label2 = spectral_clustering(X2,0.4)
plt.subplot(222)
correct2 = correct_rate(X2,y2,label2)
plt.title("noise=0.08,"+correct2)
plt.scatter(X2[:,0],X2[:,1],c=label2)

label3 = spectral_clustering(X3,0.4)
plt.subplot(223)
correct3 = correct_rate(X3,y3,label3)
plt.title("noise=0.1,"+correct3)
plt.scatter(X3[:,0],X3[:,1],c=label3)

label4 = spectral_clustering(X4,0.4)
plt.subplot(224)
correct4 = correct_rate(X4,y4,label4)
plt.title("noise=0.12,"+correct4)
plt.scatter(X4[:,0],X4[:,1],c=label4)
```

![H2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/H2.png)

## Part I

> Through experimentation, a value of epsilon is shown for which the algorithm correctly clusters the bullseye.

```python 
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![I1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/I1.png)

```python 
plt.figure(figsize=(12,8))
label1 = spectral_clustering(X,0.3)
correct1 = correct_rate(X,y,label1)
label2 = spectral_clustering(X,0.35)
correct2 = correct_rate(X,y,label2)
label3 = spectral_clustering(X,0.5)
correct3 = correct_rate(X,y,label3)
label4 = spectral_clustering(X,0.55)
correct4 = correct_rate(X,y,label4)
plt.subplot(221)
plt.title("epsilon=0.3,"+correct1)
plt.scatter(X[:,0],X[:,1],c=label1)
plt.subplot(222)
plt.title("epsilon=0.35,"+correct2)
plt.scatter(X[:,0],X[:,1],c=label2)
plt.subplot(223)
plt.title("epsilon=0.5,"+correct3)
plt.scatter(X[:,0],X[:,1],c=label3)
plt.subplot(224)
plt.title("epsilon=0.55,"+correct4)
plt.scatter(X[:,0],X[:,1],c=label4)
```
![I3.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/I3.png)

From the experiments, we are able to correctly separate the two rings when the values of epsilon from 0.3 to 0.5 roughly.

Thank you!!!