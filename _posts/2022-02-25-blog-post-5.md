---
layout: post
title: Blog Post 5
---


2022-02-25

# Image Classification

## Tensorflow

The Blog Post 5 is working in Google Colab. When training my model, enabling GPU lead to significant speed benefits.

## The Task
Classify cats and dogs

This is the link to my github respository. [https://github.com/xinyudong1129/Blog-Post/tree/main/blogpost5](https://github.com/xinyudong1129/Blog-Post/tree/main/blogpost5)

## Reference website
[Transfer leaning tutorials](https://www.tensorflow.org/tutorials/images/transfer_learning)

[Transfer leaning guide](https://tensorflow.google.cn/guide/keras/transfer_learning)

[Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/)

[Data augmentation](https://tensorflow.google.cn/tutorials/images/data_augmentation)

[Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

## Load Packages and Obtain Data

Load Packages

```python

import os
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras import datasets, layers, models,utils
from tensorflow.keras.preprocessing import image_dataset_from_directory

```

We'll use the dataset provided by the Tensorflow team that contains labeled images of cats and dogs.

```python
# location of data, code provided by professsor.
_URL = 'https://storage.googleapis.com/mledu-datasets/
        cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                           shuffle=True,
                           batch_size=BATCH_SIZE,
                           image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                           shuffle=True,
                           batch_size=BATCH_SIZE,
                           image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)

```
We constructed three datasets: training_dataset, validation_dataset and test_dataset using image_dataset_from_directory.

```python
Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
68608000/68606236 [==============================] - 1s 0us/step
68616192/68606236 [==============================] - 1s 0us/step
Found 2000 files belonging to 2 classes.
Found 1000 files belonging to 2 classes.
```

The following code is the technical code related to rapidly reading data provided by the professor.

```python

AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)

```
## 1.Visualiazation of images
We can get a piece of a dataset using the *take* method; will retrieve one batch(32 images with labels) from the training dataset.

We try to create a two-row visualizaiton. In the first row, show three random pictures of cats. In the second row, show three random pictures of dogs. Here's my code.

```python

cat = 0
dog = 0
plt.figure(figsize=(10, 6))
for images, labels in train_dataset.take(1):
  for i in range(32):
    if labels[i].numpy().astype("uint8")==0 and cat < 3:
      cat = cat + 1
      ax = plt.subplot(2, 3, cat)
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title("cat")
     
    elif labels[i].numpy().astype("uint8")==1 and dog < 3:
      dog = dog + 1
      ax = plt.subplot(2, 3, dog+3)
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title("dog")
    plt.axis("off")
    if cat >= 3 and dog >= 3:
      break

```
Here's the resutls.

![PIC1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/pic1_cat_dog.png)

## 2. First Model

We create a *tf.keras.Sequential* model named model1, which include two *Conv2D* layers, two *MaxPooling2D* layers, one *Flatten* layer, one *Dense* layer, one *Dropout* layer.

```python
model1 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.1)    
])
```

Train model1 and plot the history of the accuracy on both the training and vailidation sets.

```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model1.fit(train_dataset, 
           epochs=20, 
           validation_data=(validation_dataset))

```
Here's the history of the accuracy on both the training and vailidation sets.

```python
Epoch 1/20
63/63 [==============================] - 14s 61ms/step - loss: 16.4243 
          - accuracy: 0.5270 - val_loss: 0.8454 - val_accuracy: 0.5644
Epoch 2/20
63/63 [==============================] - 4s 55ms/step - loss: 1.1612 
        - accuracy: 0.6415 - val_loss: 0.7977 - val_accuracy: 0.6176
Epoch 3/20
63/63 [==============================] - 4s 55ms/step - loss: 1.0320 
        - accuracy: 0.7365 - val_loss: 1.1135 - val_accuracy: 0.5990
Epoch 4/20
63/63 [==============================] - 4s 54ms/step - loss: 0.8850 
        - accuracy: 0.7770 - val_loss: 1.0845 - val_accuracy: 0.6015
Epoch 5/20
63/63 [==============================] - 4s 54ms/step - loss: 0.6835 
        - accuracy: 0.8485 - val_loss: 1.3032 - val_accuracy: 0.6151
Epoch 6/20
63/63 [==============================] - 4s 58ms/step - loss: 0.6394 
        - accuracy: 0.8655 - val_loss: 1.8359 - val_accuracy: 0.5990
Epoch 7/20
63/63 [==============================] - 4s 57ms/step - loss: 0.5899 
        - accuracy: 0.8840 - val_loss: 1.6900 - val_accuracy: 0.6374
Epoch 8/20
63/63 [==============================] - 4s 54ms/step - loss: 0.4905 
        - accuracy: 0.8990 - val_loss: 1.9551 - val_accuracy: 0.6027
Epoch 9/20
63/63 [==============================] - 4s 57ms/step - loss: 0.5389 
        - accuracy: 0.8975 - val_loss: 1.8406 - val_accuracy: 0.6300
Epoch 10/20
63/63 [==============================] - 4s 58ms/step - loss: 0.4817 
        - accuracy: 0.9120 - val_loss: 1.9983 - val_accuracy: 0.6300
Epoch 11/20
63/63 [==============================] - 4s 60ms/step - loss: 0.5438 
        - accuracy: 0.8980 - val_loss: 1.9636 - val_accuracy: 0.6101
Epoch 12/20
63/63 [==============================] - 4s 56ms/step - loss: 0.5632 
        - accuracy: 0.9025 - val_loss: 2.2692 - val_accuracy: 0.6064
Epoch 13/20
63/63 [==============================] - 4s 59ms/step - loss: 0.6398 
        - accuracy: 0.8775 - val_loss: 2.0141 - val_accuracy: 0.6176
Epoch 14/20
63/63 [==============================] - 4s 59ms/step - loss: 0.4856 
        - accuracy: 0.9095 - val_loss: 2.3159 - val_accuracy: 0.6002
Epoch 15/20
63/63 [==============================] - 4s 58ms/step - loss: 0.4682 
        - accuracy: 0.9255 - val_loss: 2.6576 - val_accuracy: 0.6200
Epoch 16/20
63/63 [==============================] - 4s 60ms/step - loss: 0.5334 
        - accuracy: 0.9135 - val_loss: 2.8297 - val_accuracy: 0.6015
Epoch 17/20
63/63 [==============================] - 4s 60ms/step - loss: 0.4347 
        - accuracy: 0.9280 - val_loss: 2.7688 - val_accuracy: 0.6114
Epoch 18/20
63/63 [==============================] - 4s 60ms/step - loss: 0.4214 
        - accuracy: 0.9335 - val_loss: 2.8757 - val_accuracy: 0.6324
Epoch 19/20
63/63 [==============================] - 4s 59ms/step - loss: 0.4124 
        - accuracy: 0.9370 - val_loss: 3.0343 - val_accuracy: 0.6324
Epoch 20/20
63/63 [==============================] - 4s 59ms/step - loss: 0.4110 
        - accuracy: 0.9395 - val_loss: 3.0805 - val_accuracy: 0.6361
```

The accuracy of model1 stabilized between **85% to 94%** during training, and the validation accuracy is nearly **64%**, better than the baseline.
plot the history of model1, including the training and validation performance.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![PIC2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/pic2_model1_history.png)

## 3.Model with Data Augmentation
In this part, we add some data augmentation layers to the model. Data Augmentation refers to include modified copies of the same image in the training set. For example, we can flip the image upside down or rotate it some degrees in order to help our model learn *invariant features* of input images.

we create a *tf.keras.layers.RandomFlip()* layer and a *tf.keras.layers.randomRotation()* layer. then plot both the origianl image and a few copies to which randomflip and randomrotation has been applied.

```python
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal")
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0]/ 255)
    plt.axis('off')

```

```python
data_rotation = tf.keras.Sequential([
    tf.keras.layers.RandomRotation(0.1),
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    rotated_image = data_rotation(tf.expand_dims(first_image, 0))
    plt.imshow(rotated_image[0]/ 255)
    plt.axis('off')
```


Thank you!!!