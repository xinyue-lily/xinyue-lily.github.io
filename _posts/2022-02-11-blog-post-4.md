---
layout: post
title: Blog Post 4
---


2022-02-11

# Spectral Clustering

## Purpose

> Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. We'll write a tutorial on a simple version of the spectral clustering algorithm for clustering data points.


## Part A

> Construct the similarity matrix ð€. use a parameter epsilon. Entry A[i,j] should be equal to 1 if X[i] is within distance epsilon of X[j], and 0 otherwise. The diagonal entries A[i,i] should all be equal to zero. 


```python

def get_similarity_matrix(X,epsilon):
    """
    X: data
    epsilon:threshold
    return: similarity matrix ð€ 
    """
    dists = pairwise_distances(X, X)
    dists = np.where(dists > epsilon, 0, 1)
    np.fill_diagonal(dists,0)
    return dists

```

## Part B

> Construct the cut and volume terms. ðœð®ð­ (C0, C1) is the cut of the clusters C0 and C1. ð¯ð¨ð¥ (C) is the volume of cluster C which is a measure of the size of the cluster.

```python
def get_D(A):
    """
    A: similarity matrix ð€ 
    return: the degree Matrix of A
    """
    D = np.diag(sum(A))
    return D

def cut(A,y):
    """
    A: similarity matrix ð€ 
    y: clustering vector y
    return: the cut objective ðœð®ð­(ð¶0,ð¶1) for y
    """
    B = A[y==0,:]
    B = B[:,y==1]
    return B.sum()

def vols(A,y):
    """
    A: similarity matrix ð€ 
    y: clustering vector y
    return: computes the volumes of ð¶0 and ð¶1,returning them as a tuple.
    """
    d = np.sum(A,axis=0)
    v0 = d[y==0].sum()
    v1 = d[y==1].sum()
    return v0,v1

def normcut(A,y):
    """
    A: similarity matrix ð€ 
    y: clustering vector y
    return: the binary normalized cut objective of a matrix A with 
            clustering vector y.
    """
    v0,v1 = vols(A,y)
    Na = cut(A,y)*(1/v0+1/v1)
    return Na

```
The cut objective for the true labels is much smaller than the cut objective for the random labels, and the normcut objective for the true labels is much smaller than the normcut objective using the random labels.

## Part C
> Write a function called transform(A,y) to compute the appropriate ð³ vector.

![C1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/C1.png)

![C2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/C2.png)

```python
def transform(A,y):
    """
    A: similarity matrix ð€ 
    y: clustering vector y
    return: the appropriate ð³ vector
    """
    v0,v1 = vols(A,y)
    z = np.where(y == 0, 1/v0, -1/v1)
    return z
    
"""
check the equation above that relates the matrix product to 
the normcut objective, by computing each side separately and 
checking that they are equal.
"""
L = D - A
na2 = (z.T @ L @ z)/(z.T @ D @ z)
na4 = (z.T @ L @ z)/(z @ D @ z)
#na2 and na4 is close
#normcut(A,y) and (z.T @ L @ z)/(z.T @ D @ z) is close
print(np.isclose(na2,na4),np.isclose(na2,na))
#check the identity ð³ð‘‡ðƒðŸ™=0
na5 = z.T @ D @ np.ones(n)
print(np.isclose(na5,0))
```
True  True   

True

## Part D

> The problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function.![D2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/D2.png)
subject to the condition ð³ð‘‡ðƒðŸ™=0, 
We use the minimize function from scipy.optimize to minimize the function orth_obj with respect to  ð³ . 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)

#Use the minimize function from scipy.optimize to minimize the function 
#orth_obj with respect to ð³,the initial value is random labels yr
#return the minimizing vector z_min
#the optimization is pretty slow,almost ten minutes
z_min = scipy.optimize.minimize(orth_obj,yr)

```


![D1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/D1.png)

## Part E
> The plot shows the attempted data clustering from the optimization

```python
label = np.where(z_min.x>0,1,0)
plt.scatter(X[:,0],X[:,1],c=label)
```
![E1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/E1.png)

Just few mistakes.

## Part F
> The Laplacian matrix and its eigenvector corresponding to the second-smallest eigenvalue are both computed. The plot shows a more correct clustering of the data, with only a small number of points mis-clustered
```python
L = np.linalg.inv(D) @ (D-A)
eigval, eigvec = np.linalg.eig(L)
z_eig = eigval[1].real
print(z_eig)   
```

  z_eig = 0.002204547657770541,    z_min = 0.002211107428317444 ,  
  z_eig is close to z_min

## Part G
> We write a function called spectral_clustering(X, epsilon) which takes in the input data X (in the same format as Part A) and the distance threshold epsilon and performs spectral clustering, returning an array of binary labels indicating whether data point i is in group 0 or group 1. 

```python
def spectral_clustering(X, epsilon):
    """
    X      :input data
    epsilon:threshold param
    return :performs spectral clustering, returning an array of binary labels 
            indicating whether data point i is in group 0 or group 1. 
    """
    #Construct the similarity matrix.
    A = get_similarity_matrix(X,epsilon)
    #construct the degree matrix
    D = get_D(A)
    #Construct the Laplacian matrix
    L = np.linalg.inv(D) @ (D-A)
    #Compute the eigenvector with second-smallest eigenvalue of the 
    #Laplacian matrix.
    eigval, eigvec = np.linalg.eig(L)
    eigvec = np.real(eigvec)
    c = eigvec[:,1]
    #return labels based on this eigenvector.
    label = np.where(c>0,1,0)
    return  label

def plot(X,label):
    """
    plot the result
    X: input data
    label: the result of spectral clustering
    """
    plt.scatter(X[:,0],X[:,1],c=label)
    correct = np.isclose(y,label)
    plt.title('The correct rate:{:.0%}'.format(correct.sum()/1000) 
    
def correct_rate(X,y,label):
    """
    compute the correct rate
    X: input data
    y: true value
    label: the result of spectral clustering
    """
    correct = np.isclose(y,label)
    return 'the correct rate:{:.0%}'.format(correct.sum()/1000)

```
## Part H

> We generated different data sets using make_moons. When the noise increased, spectral clustering still find the two half-moon clusters.

```python 
n = 1000
plt.figure(figsize=(12,8))
X1,y1 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.subplot(221)
plt.title("noise=0.05")
plt.scatter(X1[:,0], X1[:,1])
X2,y2 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.08, random_state=None)
plt.subplot(222)
plt.title("noise=0.08")
plt.scatter(X2[:,0], X2[:,1])
X3,y3 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.subplot(223)
plt.title("noise=0.10")
plt.scatter(X3[:,0], X3[:,1])
X4,y4 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.12, random_state=None)
plt.subplot(224)
plt.title("noise=0.12")
plt.scatter(X4[:,0], X4[:,1]) 
```

![H1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/H1.png)


```python 
plt.figure(figsize=(12,8))

label1 = spectral_clustering(X1,0.4)
plt.subplot(221)
correct1 = correct_rate(X1,y1,label1)
plt.title("noise=0.05,"+correct1)
plt.scatter(X1[:,0],X1[:,1],c=label1)

label2 = spectral_clustering(X2,0.4)
plt.subplot(222)
correct2 = correct_rate(X2,y2,label2)
plt.title("noise=0.08,"+correct2)
plt.scatter(X2[:,0],X2[:,1],c=label2)

label3 = spectral_clustering(X3,0.4)
plt.subplot(223)
correct3 = correct_rate(X3,y3,label3)
plt.title("noise=0.1,"+correct3)
plt.scatter(X3[:,0],X3[:,1],c=label3)

label4 = spectral_clustering(X4,0.4)
plt.subplot(224)
correct4 = correct_rate(X4,y4,label4)
plt.title("noise=0.12,"+correct4)
plt.scatter(X4[:,0],X4[:,1],c=label4)
```

![H2.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/H2.png)

## Part I

> Through experimentation, a value of epsilon is shown for which the algorithm correctly clusters the bullseye.

```python 
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![I1.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/I1.png)

```python 
plt.figure(figsize=(12,8))
label1 = spectral_clustering(X,0.3)
correct1 = correct_rate(X,y,label1)
label2 = spectral_clustering(X,0.35)
correct2 = correct_rate(X,y,label2)
label3 = spectral_clustering(X,0.5)
correct3 = correct_rate(X,y,label3)
label4 = spectral_clustering(X,0.55)
correct4 = correct_rate(X,y,label4)
plt.subplot(221)
plt.title("epsilon=0.3,"+correct1)
plt.scatter(X[:,0],X[:,1],c=label1)
plt.subplot(222)
plt.title("epsilon=0.35,"+correct2)
plt.scatter(X[:,0],X[:,1],c=label2)
plt.subplot(223)
plt.title("epsilon=0.5,"+correct3)
plt.scatter(X[:,0],X[:,1],c=label3)
plt.subplot(224)
plt.title("epsilon=0.55,"+correct4)
plt.scatter(X[:,0],X[:,1],c=label4)
```
![I3.png](https://raw.githubusercontent.com/xinyue-lily/xinyue-lily.github.io/master/images/I3.png)

From the experiments, we are able to correctly separate the two rings when the values of epsilon from 0.3 to 0.5 roughly.

Thank you!!!